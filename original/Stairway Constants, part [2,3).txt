<h3>Foreword</h3>
First of all, I'm quite offended that I was trolled by an editor who replaced all occurrences of \(\frac{1}{\sqrt{2}}\) in last issue's <em>Stairway Constants, part </em><em>[1,2) </em>with \(\frac{\sqrt{2}}{2}\). The latter is superior, but not when you're trying to explain that there's two different ways to write the reciprocal of \(\sqrt{2}\).

Second, I promised I would go back with a ruler and measure the distances between tick marks on the MC north-northeast number line, so I did. At the floor 3 landing, we find the white-space between adjacent tick marks to be about 22 cm. At the floor 1.5 landing, that space is at least a few centimetres past the end of my 30 cm ruler. That's a difference of over 10 cm! The painters really did an excellent job of making the transitions seamless.

Those (SandwichExpert) in disbelief that a number line could be irregularly spaced might find consolation in a few alternate explanations:
<ol>
	<li>All this stair-climbing has made me so physically fit that I accidentally made some of the measurements while walking at relativistic speed.</li>
	<li>MC is non-Euclidean.</li>
	<li>Black holes.</li>
	<li>Witches.</li>
</ol>
If you're tuning into the Stairway Constants series for the first time, we're looking at the constants in the MC north-northeast stairwell. So far we've covered the constants in the intervals [0,1) and [1,2); this issue we'll cover [2,3). It's highly recommended that you actually go to the stairwell with this article in hand, for a fully-immersive tour.
<h3>Floor 2</h3>
Two. 2! A couple. The number of sides of this page. The number of eyes it takes to see depth. The minimum number of elements in a field. Heck, fields can't even exist without the notion of binary operators. Two distinct points define a line, and two lines are obtained by cutting it. Undoubtedly, humanity and mathematics would be impossible without the notion of grouping and dividing things into pairs -- so much so, that we <em>even</em> have words for when a number is divisible by two. 2 defines the even numbers, yet it defies them by being the only one that is prime. How odd is that?

Puns aside, there are more constants to look at...
<h3>Floor 2.5</h3>
You reach the top of the next flight of stairs (11 steps) before you can read the next constant. Are there really no interesting numbers near 2? (Quickly scouring Wikipedia, I couldn't find any seriously notable constants around 2.) <em>Exercise: invent a constant worthy enough to fill the void near 2</em><em>.</em>

You recall that the last constant we covered (in the previous issue) was around 1.6 (the golden ratio), so this gap has lasted nearly 2 flights of stairs. Let's see what constant broke the silence...

<b><i>Golden Angle</i></b>
<b><i>2.3999632297...</i></b>

(For more digits, see OEIS A131988.) A constant without a symbol? Let’s give it a symbol - how about \(\theta_G\)? It’s an angle, after all. But what is \(\theta_G\) the angle of? Let's do a thought experiment to find out.

Suppose you're a flower, and your main goal is to look pretty. Evolution has told you that the prettiest flowers appear to have their petals evenly spaced. Unfortunately, you can only grow one petal at a time. Once it's grown, you can't move it. You also don't know how many petals you will grow in your lifetime, so you better place them wisely. Even worse, plants don't have much free will. The only thing you can choose is the angle \(\theta\) between the last petal you grew, and the next. Thus, the \((n + k)\)th petal will be grown at the angle \(k \theta\), relative to the \(n\)th petal.

So what should \(\theta\) be? To ensure that your petals appear to be evenly spaced, you want to avoid having two petals that are close to one another. How might such a situation arise? Consider the \(i\)th petal and the \(j\)th petal, where \(i \neq j\). The angle between them is \((i - j) \theta\). If they appear to be close to one another, then that angle is approximately a multiple of \(2 \pi\): the number of radians in a full rotation. Then, for some integer \(k\), \((i - j) \theta \approx 2 k \pi\). If \(k = 0\), then \(\theta\) must be very small. Thus, we want to avoid small values of \(\theta\) (but intuitively, you probably already knew that). Otherwise, when \(k \neq 0\), we can rearrange:

\[\frac{2 \pi}{\theta} \approx \frac{k}{i - j}\]

In English, this means that we'll find two petals close to one another if \(\frac{2 \pi}{\theta}\), the number of times \(\theta\) fits into a full 360-degree rotation, is well-approximated by a ratio of two integers. Aha! If we make \(\frac{2 \pi}{\theta}\) a number with bad rational approximations, we can avoid that. Let's make it the famous irrational number with <em>the worst</em> rational approximations. We've seen that number before in this stairwell: it's the golden ratio \(\phi\).

\[\begin{aligned}
\frac{2 \pi}{\theta} &amp;= \phi \\
\theta &amp;= \frac{2 \pi}{\phi}
\end{aligned}\]
<h1><span style="text-decoration: underline;"><strong>[EDITORS: THE ABOVE LATEX INCLUDES AMPERSANDS. PLEASE MAKE SURE THIS DOESN'T BREAK THANKS]</strong></span></h1>
However, if you evaluate \(\frac{2 \pi}{\phi}\), you'll actually get a number around 3.88. To get the golden angle you must convert this to standard form: \(3.88... - 2 \pi = -2.39...\). Since direction doesn't matter here, that's equivalent to \(+2.39...\).

To recap, the golden angle \(\theta_G\) is explicitly tied to the golden ratio \(\phi\) by the relation \(\theta_G = 2 \pi - \frac{2 \pi}{\phi}\). It's the optimal angle (in radians) between consecutive items arranged in a circle, so that you minimize overlap.

[caption id="" align="alignnone" width="1024"]<img class="" src="https://i.imgur.com/e1fE9sx.png" alt="" width="1024" height="1024" /> Flower petals, separated by the golden angle. Image credit: Wolfgangbeyer on German Wikipedia.[/caption]

<em>Exercise: as much as we like our radians, most of us are better at visualizing angles in degrees. What’s \(\theta_G\) to the nearest degree?</em>

\(K\)<i></i><b><i>
</i></b><b><i>Khinchin's constant</i></b>
<b><i>2.685452001...</i></b>

(For more digits, see OEIS A002210.) Aleksandr Khinchin was a 20th century Soviet mathematician who proved something remarkable about continued fractions. We've seen continued fractions before in this series - the golden ratio has the infinite continued fraction:

\[\phi = 1 + \frac{1}{1 + \frac{1}{1 + \ddots}}\]

In general, simple continued fractions can be finite or infinite, and are of the form

\[a_0 + \frac{1}{a_1 + \frac{1}{\ddots + \frac{1}{a_n}}}\]

where \(a_i\) is positive for all \(i &gt; 0\). The finite continued fractions represent rational numbers, and the infinite continued fractions represent irrational numbers. Altogether, all real numbers can be expressed (almost uniquely) by a simple continued fraction.

Khinchin proved that the continued fractions for "almost all" real numbers have a very strange property. If you take the geometric mean of the coefficients (\(a_0\), \(a_1\), \(a_2\), etc), that mean will equal \(K\)[1].

"Almost all" comes with infinitely many exceptions. For starters, we're pretty sure that no rational number has this property. (The irrationals far outnumber the rationals, so this is okay.) More interesting exceptions include the golden ratio, where \(a_i = 1\) for all \(i\). In that case, the geometric mean is obviously 1.

Khinchin's constant itself also has a continued fraction expansion. The coefficients go 2, 1, 2, 5, 1, 1, 2, ... (for more terms, see OEIS A002211). However, we don't actually know if their geometric mean is \(K\). In fact, we don't even know if \(K\) is rational.

<em>Exercise: here's a throwback to the first stairway constant. What is the continued fraction for Liouville's constant? Does the geometric mean of its coefficients work out to \(K\)?</em>

\(e\)<i></i><b><i>
</i></b><b><i>Napier's constant (Euler's number)</i></b>
<b><i>2.71828182845...</i></b>

(For more digits, see OEIS A001113.) Chances are that you recognized this number by its symbol, rather than its name. Maybe you were even waiting to see it on this number line. That's how you know this is <em>some</em> constant. We don't use \(e\) for anything else, because that would be an insult to the one true \(e\).

Why \(e\)? Euler was the first to use that symbol for this number, but his choice of \(e\) was probably not in honour of himself. When you're churning out papers like Leonhard Euler, you'll gladly take the first letter you haven't yet used for anything else. Before Euler, the number was also known as \(b\). Nowadays, in honour of Euler, we use his notation. The fact that the notation happens to be the first letter of "<span style="text-decoration: underline;">E</span>uler" and "<span style="text-decoration: underline;">e</span>xponential" is probably just a happy coincidence[2].

Indeed, Euler did not discover Euler's number. John Napier, the Scottish inventor of logarithms, published a table of values for the natural logarithm function \(\ln\) almost a century before Euler was born. Since \(e\) is the base of the natural logarithm (\(\ln = \log_e\)), Napier is considered to be the first to observe the special properties of \(e\) (hence the name Napier's constant). Euler comes into the picture with his many discoveries about \(e\). Most notably, Euler's identity is the famous equation \(e^{i \pi} = -1\)[3].

How'd he come up with that? Short answer: Maclaurin series. The Maclaurin series is also how Euler computed \(e\) to 18 decimal places by hand! You can try it yourself:

\[e^x = \sum_{i=0}^{\infty} \frac{x^i}{i!}\]

A complete list of the properties of \(e\) could probably fill a book thicker than this stairwell is tall. You yourself are familiar with many of them. Here's a relatively accessible one from statistics you may or may not know: derangements. If you randomly shuffle a deck of \(n\) cards, the probability that no card ends up in its original position approaches \(\frac{1}{e}\) as \(n\) increases to infinity. Of course, the proof of this result involves the Maclaurin series above. <em>Exercise: fill in the details of the proof.</em>
<h3>Floor 3</h3>
Another 11 steps later, you reach a big pink 3. The third floor! The hub of math hubbub. It's home to the MC-DC bridge (lower half), the Comfy, the C&amp;D, MathSoc, and of course the filthy mathNEWS office. Perhaps one day it may also have a bridge to SLC, but for now the shit's <em>still</em> broken.

Hiding above the door to the 3rd floor is this article's final constant.

\(F\)<i></i><b><i>
</i></b><b><i>Fransén-Robinson constant</i></b>
<b><i>2.8077702420...</i></b>

(For more digits, see OEIS A058655.) This constant is relatively new. It seems to have emerged in Arne Fransén's 1979 paper <em>Accurate determination of the inverse gamma integral</em>. Wikipedia mentions a Herman P. Robinson, which is the name of a late OEIS contributor and co-author of the report <em>Mathematical constants</em> (Lawrence Radiation Laboratory, University of California, 1971). It's likely, but I'm not sure he is the Robinson after whom \(F\) is named.

As the title of Arne Fransén's paper suggests, \(F\) is the value of an integral of the inverse gamma function:

\[F = \int_{0}^{\infty} \frac{1}{\Gamma(x)} dx\]

In English, it's the area bounded above by the graph of the reciprocal of the gamma function and bounded below by the \(x\)-axis. If you're unfamiliar with the gamma function \(\Gamma\), it is the continuous function where \(\Gamma(n) = (n - 1)!\) for all positive integers \(n\). Essentially, it's what you get when you draw a curve of best fit through the points \((n, (n - 1)!)\). \(\Gamma(n)\) grows <em>really</em> fast - it's beyond exponential. (Next time someone uses the word "exponential" incorrectly, you can show them the gamma function and say that exponential growth is for babies.)

As a consequence of that growth, \(\frac{1}{\Gamma(x)}\) approaches zero <em>really </em>fast. This is how \(K\) can be finite; it's the area between the \(x\)-axis and the graph of a function that pretty much kisses it.

<em>Exercise: use lower Riemann sums to prove that \(F \geq e\).</em>

<hr />

Halfway through the term, we have reached just short of halfway up the MC north-northeast stairwell. However, we have covered more than half of the stairway constants. Why is that?

We're not the first ones to notice. Simon Newcomb and Frank Albert Benford Jr. both observed this trend more than 80 years ago: numbers, in practice, tend to have small leading digits. This is known as the Newcomb-Benford law. It's not a theorem, but rather a highly-applicable pattern for all sorts of real-world ("realistically-distributed") data. In fact, the Newcomb-Benford law holds so reliably that it can and has been used to detect financial fraud!

Out of the numbers between 1 and 7, we've covered the leading digits 1 and 2 so far. According to the Newcomb-Benford law, almost 48% of all "realistically-distributed" numbers should begin with 1 or 2 (on average, of course). The rest of the constants we have yet to cover in this stairway begin with 3, 4, 5, or 6; those digits account for less than 37% of all "realistically-distributed" numbers. If you believe that stairway constants have a "realistic" distribution (whatever that means), then it should be no surprise that there are more of them on the lower floors.

Of course, there is also a bias towards smaller numbers in mathematics. We care much more about extremes, and many of the stairway constants are significant in the first place, because they are the smallest or the simplest number to satisfy some condition.

The only way to truly know why the stairway constants are distributed the way they are, is to ask the people who created this number line in the first place. Stay tuned for more developments.
<p style="text-align: right;">water</p>
<em>Exercise: don't take the elevator.</em>

[1]For infinite continued fractions, you have to take the limit as \(n\) goes to infinity of the geometric mean of the first \(n\) coefficients: \(\lim_{n \to \infty} \sqrt[n]{a_0 a_1 a_2 \dots a_{n-1}}\).

[2] http://mathshistory.st-andrews.ac.uk/HistTopics/e.html

[3] Whether Euler explicitly made this discovery is questionable, but it follows immediately from a more general equation called Euler's formula.
